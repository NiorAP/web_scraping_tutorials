{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python Web Scraping #3: ใช้ Beautiful Soup ในการอ่านข้อมูลในหน้า Website"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "by Nior Atthakorn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "จาก[ตอนที่แล้ว](https://medium.com/@nioratthakorn/python-web-scraping-tutorial-2-7a8d09a36093) เราได้ใช้ [Requests](https://pypi.org/project/requests/) ร่วมกับ [Pandas](https://pandas.pydata.org) ในการทำ Web Scraping มาแล้ว ซึ่งก็ยังคงมีข้อจำกัดอยู่ว่าข้อมูลที่ต้องการจะต้องอยู่ในรูปแบบตารางเท่านั้น ซึ่งในหลายๆ ครั้งข้อมูลที่เราต้องการก็ไม่ได้อยู่ในรูปแบบที่เราต้องการนั้น\n",
    "\n",
    "ในบทความนี้เราจึงจะใช้ [Beautiful Soup](https://pypi.org/project/beautifulsoup4/) ซึ่งเป็น library สำหรับการ parse HTML ให้อยู่ในรูปแบบที่เข้าถึงได้ง่ายขึ้น มาช่วยแก้ปัญหาที่ตัว Pandas สามารถแกะได้เฉพาะข้อมูลที่เป็นตารางเท่านั้น และให้ Pandas ไปเป็นปลายทางของข้อมูล (เป็นตารางที่เก็บข้อมูลหลังจากทำเสร็จแล้ว) แทน จะมีวิธีการทำยังไง ก็อ่านต่อด้านล่างนี้เลย Let's Go!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table align=\"center\">\n",
    "    <td>\n",
    "        <a href=\"https://github.com/NiorAP/web_scraping_tutorials/blob/master/Python%20Web%20Scraping%20%233/Python%20Web%20Scraping%20%233.ipynb\">\n",
    "            <img src=\"https://pngimg.com/uploads/github/github_PNG58.png\" width=\"100\"/>\n",
    "            <center>\n",
    "                ดูใน GitHub\n",
    "            </center>\n",
    "        </a>\n",
    "    </td>\n",
    "    <td>\n",
    "        <a href=\"https://colab.research.google.com/\">\n",
    "            <img src=\"https://colab.research.google.com/img/colab_favicon_256px.png\" width=\"100\"/>\n",
    "            <center>\n",
    "                ลองเล่นใน Google Colab\n",
    "            </center>\n",
    "        </a>\n",
    "    </td>\n",
    "    <td>\n",
    "        <a href=\"https://medium.com/@nioratthakorn/python-web-scraping-3-f3591628f168\">\n",
    "            <img src=\"https://cdn4.iconfinder.com/data/icons/vector-brand-logos/40/Medium-512.png\"/ width=\"100\">\n",
    "            <center>\n",
    "                อ่านใน Medium\n",
    "            </center>\n",
    "        </a>\n",
    "    </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ก่อนหน้านี้เราใช้ function [`get`](https://requests.readthedocs.io/en/master/user/quickstart/#make-a-request) ของ Requests ในการดึงข้อมูล HTML จาก website จากนั้นให้ function `read_html` ของ Pandas จัดการดึงตารางออกมาจาก HTML ที่ได้ แต่ถ้าข้อมูลส่วนที่เราต้องการไม่ใช่ตารางหล่ะ ?\n",
    "\n",
    "ถ้ายังจำกันได้ จริงๆ แล้ว Pandas เป็น library ที่ถูกสร้างขึ้นมาเพื่อใช้ในการ manipulate data เป็นหลัก แต่ก็มีความสามารถที่จะนำมาใช้ scrape ได้ตามบทความในตอนที่ 1 และ 2 ด้วยความสามารถในส่วนของการ read data ของ Pandas\n",
    "\n",
    "แต่ถ้า data ของเราไม่ได้มาในรูปแบบของตารางโดยตรง (อาจจะไม่เป็นตารางเลย หรือเรามองเป็นตารางแต่ HTML ที่ผู้เขียน website นั้นใช้ไม่ใช่รูปแบบของตาราง) เราก็จะไม่สามารถใช้ Pandas ในการ Scrape ได้แล้ว ในบทความนี้จึงจะให้ Pandas กลับไปทำหน้าที่ manipulate data เหมือนเดิม\n",
    "\n",
    "แล้วเราจะใช้อะไรมาจัดการแทน ? เดาไม่ยากครับ ก็ใช้ Beautiful Soup มาช่วยในการ parse ตัว HTML ที่ requests ดึงมาแทน Pandas แล้วนำไปเก็บใน `DataFrame` ของ Pandas เพื่อที่จะนำไป manipulate ต่อได้ง่ายๆ นั่นเองงงง\n",
    "\n",
    "<i><center><b>ในเมื่อ Pandas ดึงข้อมูลได้แค่ตารางจากหน้า website ก็ให้ Beautiful Soup มาดึงข้อมูลแบบอื่นๆ แล้วค่อยจัดการด้วย Pandas ต่อ</b></center></i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i><b>ขั้นตอนที่ 1</b></i> ก็เหมือนๆ เดิมครับ `import` library ที่จะใช้ ในที่นี้จะใช้ 3 ตัว\n",
    "- Requests -> ดึงข้อมูลหน้า website\n",
    "- Beautiful Soup -> parse HTML ให้เราหา component ต่างๆ ได้ง่าย (และยืดหยุ่น) ขึ้น\n",
    "- Pandas -> manipulate data ที่ได้"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WVqfaFZ2ar3o"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i><b>ขั้นตอนที่ 2</b></i> ใส่ url ของ website ที่เราต้องการ scrape เหมือนเดิม ในที่นี้จะใช้ website [Meta tier list ของเกม LoR](https://lor.mobalytics.gg/meta-tier-list) ของ [mobalytics](https://lor.mobalytics.gg) ต้องขออนุญาตและขอขอบคุณมา ณ ที่นี้ด้วยนะครับ -/\\\\-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://lor.mobalytics.gg/meta-tier-list'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i><b>ขั้นตอนที่ 3</b></i> ให้เราใช้ function `get` ของ Requests ในการดึงข้อมูลหน้า website โดยใส่ parameter เป็น url ที่เราได้กำหนดไว้ในขั้นตอนที่แล้ว (เหมือน[บทความที่แล้ว](https://medium.com/@nioratthakorn/python-web-scraping-tutorial-2-7a8d09a36093#9142)เป๊ะๆ เลย)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i><b>ขั้นตอนที่ 4</b></i> สร้าง object [`BeautifulSoup`](https://www.crummy.com/software/BeautifulSoup/bs4/doc/#making-the-soup) โดยใส่ argument เป็น HTML text (response.text เหมือนเดิมที่ได้กล่าวไปแล้วใน[ขั้นตอนที่ 4 ในบทความที่ 2]()) คู่กับ parser ที่จะใช้ ซึ่งในที่นี้เราต้องการ parse HTML ก็ใช้ \"html.parser\" เลยยยย (สำหรับเรื่อง parser สามารถดูเพิ่มเติมได้[ที่นี่](https://www.crummy.com/software/BeautifulSoup/bs4/doc/#installing-a-parser)เลยครับ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(response.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ก่อนจะไปถึงขั้นตอนที่ 5 เรามาดูหน้าตาของ website ที่เราต้องการจะ scrape สักนิดนึงว่าเป็นยังไง และเราต้องการดึงส่วนไหนออกมา\n",
    "\n",
    "![](Image-3-1.png)\n",
    "<center><i>ภาพที่ 1: หน้า website ที่เราต้องการจะ scrape</i></center>\n",
    "\n",
    "ในบทความนี้เราจะลองดึง\n",
    "- Tier ของ Deck (ตัวอักษรที่มุมบนซ้ายของแต่ละช่อง)(กรอบสีแดง)\n",
    "- ชื่อ Deck (ตัวอักษรตรงกลางของแต่ละช่อง)(กรอบสีเขียว)\n",
    "\n",
    "ในการจะดึงข้อมูลจากหน้า website เราต้องเริ่มจากการหา tag ที่ข้อมูลของเราอยู่ให้ได้ก่อน เพื่อจะได้ทำการดึง tag นั้นพร้อมข้อมูลที่เราต้องการออกมาได้\n",
    "\n",
    "โดย ก่อนจะหา tag เราต้องเปิดหน้า [HTML Elements](https://en.wikipedia.org/wiki/HTML) ขึ้นมาดูก่อน (ใครที่ไม่มีพื้นฐาน HTML เลย อาจจะงงๆ นิดนึง) สำหรับใน Google Chrome สามารถทำได้โดยการคลิกขวา แล้วเลือก Inspect (กรอบสีแดงตามภาพที่ 2)\n",
    "\n",
    "![](Image-3-2.png)\n",
    "<center><i>ภาพที่ 2: คลิกขวาแล้วเลือก Inspect</i></center>\n",
    "\n",
    "จากนั้นจะมีหน้าต่างที่เป็น HTML Elements ขึ้นมาทางด้านล่าง ดังภาพที่ 3 (ของใครขึ้นด้านข้างก็ไม่ต้องตกใจไปครับ เป็น HTML Elements แบบนี้เหมือนกันก็พอครับ)\n",
    "\n",
    "![](Image-3-3.png)\n",
    "<center><i>ภาพที่ 3: หน้า browser หลังจากเปิดหน้าต่าง HTML Elements</i></center>\n",
    "\n",
    "จากหน้านี้เราสามารถนำเมาส์ไปชี้ที่ tag ทางด้านล่าง เพื่อดูว่า tag นั้นครอบคลุมอะไรบ้างได้ ดังภาพที่ 4\n",
    "\n",
    "![](Image-3-4.png)\n",
    "<center><i>ภาพที่ 4: Highlight ส่วนที่ตรงกับ tag ที่เราเอา mouse ชี้อยู่</i></center>\n",
    "\n",
    "ในภาพที่ 4 จะเห็นว่าพอลองเอาเมาส์ชี้ไปที่ tag ในกรอบสีแดง จะมี highlight ขึ้นที่หน้า website ตรงกรอบสีเขียว เราก็จะใช้เทคนิคตรงนี้ในการหาส่วนที่เราต้องการ เพื่อนำไปใช้ในการ scrape ได้\n",
    "\n",
    "หลังจากเรารู้เทคนิคแล้ว ก็ได้เวลาไปต่อข้อ 5 กันจริงๆ แล้ว goๆ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i><b>ขั้นตอนที่ 5</b></i> เนื่องจาก Tier ของ Deck (ตัวอักษรมุมบนซ้ายของช่อง) กับชื่อ Deck (ตัวอักษรตรงกลางของช่อง) มีสิ่งที่เหมือนกันคือ อยู่ใน \"ช่อง\" เดียวกัน ดังนั้นในขั้นตอนที่ 5 นี้เราจะทำการหาช่องก่อน ให้เราลองเลื่อนเมาส์ในหน้า HTML Elements ไปเรื่อยๆ จนเจอช่องที่เราต้องการนะครับ\n",
    "\n",
    "HTML Elements จะมีลักษณะเป็น hierarchy นะครับ อย่างตอนนี้เรารู้ tag ที่ highlight ตัวอักษรตรงกลางกรอบที่เราต้องการแล้ว เราก็ค่อยๆ ขยับขึ้น (ขยับไปหา tag ที่มี hierarchy สูงกว่า) แล้วดูว่าได้กรอบที่ต้องการหรือยัง โดยอย่าลืมว่ากรอบที่เราต้องการต้องครอบคลุมตัวอักษรตรงกลางและตัวอักษรตรงมุมบนซ้ายด้วย\n",
    "\n",
    "![](Image-3-5.png)\n",
    "<center><i>ภาพที่ 5: Highlight ช่องแรกที่เราต้องการ</i></center>\n",
    "\n",
    "พอเจอกรอบแล้วอย่าลืมลองเลื่อนๆ ดู tags ที่อยู่ด้านในด้วยนะครับ ว่ามีครบที่เราต้องการจริงๆ มั้ย อย่างในรูปที่ 5 นี้ผมเช็คแล้วว่ามีครบที่เราต้องการแน่นอน\n",
    "\n",
    "ในที่นี้เรารู้แล้วว่า ข้อมูลที่เราต้องการนั้นอยู่ใน tag \"div\" แต่ว่า tag \"div\" นั้นมีเยอะเหลือเกิน พร้อมทั้งอยู่ในหลายชั้นเหลือเกิน การดึงข้อมูลจาก tag \"div\" ทั้งหมดนั้นอาจจะไม่ค่อยสะดวกนัก แต่ไม่ต้องตกใจไป ค่อยๆ ดูไปก่อน\n",
    "\n",
    "เริ่มจากการลองยุบ tag \"div\" ที่เราเจอนี้ลงก่อน (ดังภาพที่ 6) เพื่อจะได้ลองดูว่า กรอบอื่นๆ ที่เราต้องการเหมือนกันเนี่ย มีความเหมือน/ต่างอย่างไรกับ tag นี้บ้าง\n",
    "\n",
    "![](Image-3-6.png)\n",
    "<center><i>ภาพที่ 6: tag ของแต่ละช่องมีส่วนที่เหมือนกัน</i></center>\n",
    "\n",
    "จะเห็นว่า แต่ละกรอบที่เราต้องการนั้น มีสิ่งหนึ่งที่เหมือนกันคือ มีชื่อ class ที่เหมือนกัน เราจะใช้ตรงนี้เป็นตัวช่วยในการดึงข้อมูลที่เราต้องการกัน\n",
    "\n",
    "ให้เราใช้ method `.find_all` ของ BeautifulSoup object ในการหา tag ที่มีลักษณะตามที่เราต้องการทั้งหมด โดยหลักๆ แล้วเราจะใส่ argument กัน 2 ตัว นั่นคือ\n",
    "- name: ชื่อของ tag ที่เราจะดึงข้อมูล\n",
    "- attrs: [dictionary](https://docs.python.org/3/tutorial/datastructures.html#dictionaries) ที่มี key เป็นชื่อของ attribute และ value เป็นค่าของ attribute นั้นๆ\n",
    "\n",
    "อ่านแล้วอาจจะยังงง ลองดูตัวอย่างที่เรากำลังจะทำนี้ครับ ชื่อของ tag ของเราคือ `\"div\"` แล้ว attrs ของเรามี 1 คู่ คือมี key เป็น `\"class\"` และ value เป็น `\"meta-archetypecomponent__MetaArchetypeWrapper-sc-10x0k3h-0 grJXaq\"` นั่นเองครับ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = soup.find_all('div', {'class': \n",
    "    'meta-archetypecomponent__MetaArchetypeWrapper-sc-10x0k3h-0 grJXaq'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ผลลัพธ์ที่เราได้จาก `.find_all` นั้น จะเป็น list ของ tag (ครอบคลุมถึง tag ย่อยๆ ทั้งหมด) ที่ตัว method สามารถหาเจอนะครับ\n",
    "\n",
    "ทีนี้หลังจากเราได้ผลลัพธ์แล้ว เราลองมาดูกันสักนิดว่าเราได้จำนวนของกรอบเท่ากับในหน้าเว็บหรือไม่ ณ วันที่เขียนบทความนี้ มีกรอบทั้งหมด 18 กรอบ เราก็ลองใช้ function `len` ดูว่าผลลัพธ์ของเรามีเท่ากันหรือไม่"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "สวยงามมมม ผลลัพธ์ก็ได้ตรงตามหน้า website พอดี"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i><b>ขั้นตอนที่ 6</b></i> หลังจากได้กรอบทั้งหมดมาแล้ว เราจะต้องเอา tag ที่ได้ของแต่ละกรอบ มาวน [loop](https://docs.python.org/3/tutorial/controlflow.html#for-statements) เพื่อหา Tier ของ Deck (ตัวอักษรมุมบนซ้ายของช่อง) และชื่อ Deck (ตัวอักษรตรงกลางของช่อง) ออกมา แต่ก่อนจะวน loop เราจะลองเอา tag แรกของเรามาหาก่อน จะได้รู้ว่าใน loop จะต้องดึงยังไง\n",
    "\n",
    "<i>6.1</i> เริ่มจากการดึงผลลัพธ์ตัวแรกออกมา"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_result = results[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ลองหา tag ของตัวอักษรมุมบนซ้าย โดยการคลิกขวาแล้วเลือก Inspect อีกครั้งที่ตัวอักษรนั้น หน้า HTML Elements ข้างล่างจะเด้งไปหาส่วนที่เราต้องการเอง จะได้ผลลัพธ์ดังภาพ\n",
    "\n",
    "![](Image-3-7.png)\n",
    "<center><i>ภาพที่ 7: tag ของตัวอักษรมุมบนซ้าย</i></center>\n",
    "\n",
    "จะเห็นว่าตัวอักษรที่เราต้องการนั้นอยู่ใน tag \"span\" และมี class เป็น \"tier-hex-imagestyles__TierLabel-sc-13fs6l0-2 bTGiWh\" เราก็ลองใช้ method `.find_all` เหมือนเดิมก่อน เพื่อลองดูว่าหาเจอไหม และมีอะไรอย่างอื่นติดมาไหม"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<span class=\"tier-hex-imagestyles__TierLabel-sc-13fs6l0-2 bTGiWh\">S</span>]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_result.find_all('span', \n",
    "    {'class': 'tier-hex-imagestyles__TierLabel-sc-13fs6l0-2 bTGiWh'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "จะเห็นว่า มีแค่ tag ที่เราต้องการเพียงอย่างเดียวเลย เราสามารถเปลี่ยนมาใช้ method `.find` เฉยๆ แทน `.find_all` ได้ ก็จะได้ผลลัพธ์เป็น tag แรกที่เจอตามรูปแบบที่เราค้นหา (แต่ในที่นี้มี tag เดียวที่เราต้องการอยู่แล้ว ไม่มีปัญหาที่จะใช้ `.find` เฉยๆ)\n",
    "\n",
    "นอกจากนี้เราต้องการแค่เพียงตัวอักษรที่อยู่ข้างใน (ในกรอบแรกนี้คือตัว \"S\") เพียงอย่างเดียว ไม่ได้ต้องการทั้ง tag ยาวๆ เราจะใช้ attribute `.text` ในการดึงข้อความข้างในออกมา ทำให้ได้ผลลัพธ์ตามนี้"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'S'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_result.find('span', \n",
    "    {'class': 'tier-hex-imagestyles__TierLabel-sc-13fs6l0-2 bTGiWh'}).text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ในส่วนของข้อความตรงกลางช่อง เราก็ใช้วิธีเดียวกันกับตัวอักษรมุมบนซ้าย จะได้ผลลัพธ์ดังนี้"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Heimer Control'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_result.find('h3').text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "สังเกตว่าตอนใช้ method `.find` ในครั้งนี้ผมไม่มีการใส่ attribute แต่อย่างที่บอกไปก่อนหน้านี้แล้วว่า เราจะใส่/ไม่ใส่ยังไงก็ได้ ถ้าเท่าที่เราใส่ทำให้เราสามารถเข้าถึงข้อมูลที่เราต้องการได้ เท่านั้นก็เพียงพอแล้ว (ก็ในข้อบนๆ จริงๆ ก็สามารถใส่ attribute \"id\" ได้ด้วยนี่นา แต่ไม่มีประโยชน์ในการหา ก็ไม่ใส่เหมือนกันแหละคร้าบ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>6.2</i> ทีนี้หลังจากเราได้แล้วว่า\n",
    "- ส่วนของ Tier ของ Deck (ตัวอักษรมุมบนซ้ายของช่อง) สามารถหาได้โดยเอา tag ของช่องนั้นๆ มา `.find('span', {'class': 'tier-hex-imagestyles__TierLabel-sc-13fs6l0-2 bTGiWh'}).text`\n",
    "- ส่วนของชื่อ Deck (ตัวอักษรตรงกลางของช่อง) สามารถหาได้โดยเอา tag ของช่องนั้นๆ มา `.find('h3').text`\n",
    "\n",
    "เราก็มาทำการ loop เพื่อเก็บข้อมูลลง `DataFrame` กัน แต่ก่อนจะ Loop ให้เราสร้าง `DataFrame` เปล่าๆ ขึ้นมา เพื่อรอการเพิ่มข้อมูลจาก loop กัน"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(columns=['Deck Tier', 'Deck Name'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "หลังจากนั้นก็ทำการ loop เพื่อดึงข้อมูลที่เราต้องการมาใส่ใน `DataFrame` เลยยยยย"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop tag ของแต่ละช่องที่เราหาได้\n",
    "for tag in results:\n",
    "    \n",
    "    # สร้าง dictionary เปล่าเพื่อเก็บข้อมูลช่องที่กำลังดู\n",
    "    temp = {}\n",
    "    \n",
    "    # หา Tier ของ Deck (ตัวอักษรมุมบนซ้ายของช่อง) แล้วเก็บค่าใน dict\n",
    "    temp['Deck Tier'] = tag.find('span', \n",
    "        {'class': 'tier-hex-imagestyles__TierLabel-sc-13fs6l0-2 bTGiWh'}).text\n",
    "    \n",
    "    # หาชื่อ deck แล้วเก็บค่าใน dict\n",
    "    temp['Deck Name'] = tag.find('h3').text\n",
    "    \n",
    "    # นำ dict ที่มีไปเพิ่มข้อมูลใน DataFrame ที่สร้างไว้\n",
    "    df = df.append(temp, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "หลังจาก loop เสร็จเรียบร้อยแล้ว ก็มาดูผลลัพธ์กัน"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Deck Tier</th>\n",
       "      <th>Deck Name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>S</td>\n",
       "      <td>Heimer Control</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>S</td>\n",
       "      <td>Tempo Sejuani</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>S</td>\n",
       "      <td>Deep Monsters</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A</td>\n",
       "      <td>Burn Aggro</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A</td>\n",
       "      <td>Scouts</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>A</td>\n",
       "      <td>Sejuani Vlad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>A</td>\n",
       "      <td>Midrange Bannerman</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>A</td>\n",
       "      <td>Ezreal Karma</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>A</td>\n",
       "      <td>Midrange Shen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>A</td>\n",
       "      <td>Endure Spiders</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>A</td>\n",
       "      <td>Kinkou Elusives</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>A</td>\n",
       "      <td>Lux Karma</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>A</td>\n",
       "      <td>Swain TF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>B</td>\n",
       "      <td>Keg Control</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>B</td>\n",
       "      <td>Yasuo Control</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>B</td>\n",
       "      <td>Corina Control</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>B</td>\n",
       "      <td>Sejuani TF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>B</td>\n",
       "      <td>Sejuani Swain</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Deck Tier           Deck Name\n",
       "0          S      Heimer Control\n",
       "1          S       Tempo Sejuani\n",
       "2          S       Deep Monsters\n",
       "3          A          Burn Aggro\n",
       "4          A              Scouts\n",
       "5          A        Sejuani Vlad\n",
       "6          A  Midrange Bannerman\n",
       "7          A        Ezreal Karma\n",
       "8          A       Midrange Shen\n",
       "9          A      Endure Spiders\n",
       "10         A     Kinkou Elusives\n",
       "11         A           Lux Karma\n",
       "12         A            Swain TF\n",
       "13         B         Keg Control\n",
       "14         B       Yasuo Control\n",
       "15         B      Corina Control\n",
       "16         B          Sejuani TF\n",
       "17         B       Sejuani Swain"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ดูดีมีชาติตระกูลลลล"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "เป็นไงกันบ้างครับกับการใช้ Beautiful Soup มาช่วย Requests ทำ Web Scraping ก็ยังง่ายเหมือนเดิมใช่ไหมหล่ะครับ ทีนี้เราก็เหมือนจะ Scrape website ไหนก็ได้แล้ว เย่ๆ\n",
    "\n",
    "อย่าเพิ่งดีใจไปครับ จริงๆแล้วทั้งหมดที่เราทำมานี้ สามารถใช้ได้กับ website ที่เป็น [static](https://en.wikipedia.org/wiki/Static_web_page) เท่านั้น ถ้าเป็น [dynamic](https://en.wikipedia.org/wiki/Dynamic_web_page) เราจะไม่สามารถดึงข้อมูลบางส่วน (หรืออาจจะทั้งหมดเลย) ได้ แล้วเราจะทำยังไงดี ติดตามได้ในตอนต่อไปครับ\n",
    "\n",
    "ส่วนใครยังไม่ได้อ่าน 2 ตอนก่อนหน้านี้ หรืออ่านแล้วลืมแล้ว สามารถย้อนอ่านได้ที่นี่เลย:\n",
    "- [Python Web Scraping #1: ใช้ Pandas ดึงข้อมูลที่เป็นตารางจากหน้า Website](https://medium.com/@nioratthakorn/python-web-scraping-tutorial-1-9cba93ac2690)\n",
    "- [Python Web Scraping #2: ใช้ Requests มาช่วย Pandas ดึงข้อมูล](https://medium.com/@nioratthakorn/python-web-scraping-tutorial-2-7a8d09a36093)\n",
    "\n",
    "สำหรับวันนี้ขอตัวก่อนนะคร้าบ บายยยย"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Scrape#3.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
